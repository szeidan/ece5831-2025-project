{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29017ef",
   "metadata": {},
   "source": [
    "# LeNet in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f00a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "190ccf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "532e3441",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.Input(shape=input_shape),\n",
    "    keras.layers.Conv2D(filters=6, kernel_size=(5, 5), activation='sigmoid', input_shape=input_shape, padding='same'),\n",
    "    keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "    keras.layers.Conv2D(filters=16, kernel_size=(5, 5), activation='sigmoid', padding='valid'),\n",
    "    keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(120, activation='sigmoid'),\n",
    "    keras.layers.Dense(84, activation='sigmoid'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6f38a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_6             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_7             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,164</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">850</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │           \u001b[38;5;34m156\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_6             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m2,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_7             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)            │        \u001b[38;5;34m48,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)             │        \u001b[38;5;34m10,164\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m850\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "016c64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e308e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the images to the range [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b781e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a channel dimension to the images\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a887265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "y_train shape: (60000,)\n",
      "x_test shape: (10000, 28, 28, 1)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# print out the shapes of the datasets\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2242523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels to one-hot encoding\n",
    "#y_train = keras.utils.to_categorical(y_train, num_classes=10) \n",
    "#y_test = keras.utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7a90bf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "053194e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7221 - loss: 0.8619 - val_accuracy: 0.9262 - val_loss: 0.2528\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9383 - loss: 0.2030 - val_accuracy: 0.9565 - val_loss: 0.1427\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9595 - loss: 0.1330 - val_accuracy: 0.9685 - val_loss: 0.1055\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9689 - loss: 0.1019 - val_accuracy: 0.9758 - val_loss: 0.0819\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9750 - loss: 0.0826 - val_accuracy: 0.9765 - val_loss: 0.0779\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9781 - loss: 0.0697 - val_accuracy: 0.9786 - val_loss: 0.0716\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9809 - loss: 0.0620 - val_accuracy: 0.9818 - val_loss: 0.0605\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9832 - loss: 0.0543 - val_accuracy: 0.9778 - val_loss: 0.0691\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9850 - loss: 0.0481 - val_accuracy: 0.9823 - val_loss: 0.0575\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9869 - loss: 0.0424 - val_accuracy: 0.9836 - val_loss: 0.0570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ca889a4170>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start training the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6a117001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model as lenet_keras.h5\n"
     ]
    }
   ],
   "source": [
    "# save the trained model for module9.py\n",
    "model.save(\"lenet_keras.h5\")\n",
    "print(\"Saved model as lenet_keras.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34d9dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9832 - loss: 0.0561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05609326809644699, 0.9832000136375427]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the labels for test dataset\n",
    "model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "663ccd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4d37e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "prediction_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64588bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a325c3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71bab2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d9727a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total misclassified samples: 168 out of 10000\n"
     ]
    }
   ],
   "source": [
    "wrong_count = 0\n",
    "\n",
    "for i, label in enumerate(prediction_labels):\n",
    "    if label != y_test[i]:\n",
    "        wrong_count += 1\n",
    "        #print(f\"Misclassified sample index: {i}, Predicted label: {label}, True label: {y_test[i]}\")   \n",
    "        # \n",
    "print(f\"Total misclassified samples: {wrong_count} out of {len(y_test)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b17bbdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure(240x240)\n",
      "Success: Image 5_4.png is for digit 5 and the inference result is 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 10:54:41.100587: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-02 10:54:43.726786: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-02 10:54:45.699604: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "!python module9.py 5_4.png 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34281f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Image 0_0.png is for digit 0 and the inference result is 0.\n",
      "Success: Image 0_1.png is for digit 0 and the inference result is 0.\n",
      "Fail: Image 0_2.png is for digit 0 but the inference result is 8.\n",
      "Success: Image 0_3.png is for digit 0 and the inference result is 0.\n",
      "Success: Image 0_4.png is for digit 0 and the inference result is 0.\n",
      "Success: Image 1_0.png is for digit 1 and the inference result is 1.\n",
      "Success: Image 1_1.png is for digit 1 and the inference result is 1.\n",
      "Success: Image 1_2.png is for digit 1 and the inference result is 1.\n",
      "Success: Image 1_3.png is for digit 1 and the inference result is 1.\n",
      "Success: Image 1_4.png is for digit 1 and the inference result is 1.\n",
      "Success: Image 2_0.png is for digit 2 and the inference result is 2.\n",
      "Success: Image 2_1.png is for digit 2 and the inference result is 2.\n",
      "Success: Image 2_2.png is for digit 2 and the inference result is 2.\n",
      "Success: Image 2_3.png is for digit 2 and the inference result is 2.\n",
      "Success: Image 2_4.png is for digit 2 and the inference result is 2.\n",
      "Success: Image 3_0.png is for digit 3 and the inference result is 3.\n",
      "Success: Image 3_1.png is for digit 3 and the inference result is 3.\n",
      "Success: Image 3_2.png is for digit 3 and the inference result is 3.\n",
      "Success: Image 3_3.png is for digit 3 and the inference result is 3.\n",
      "Success: Image 3_4.png is for digit 3 and the inference result is 3.\n",
      "Success: Image 4_0.png is for digit 4 and the inference result is 4.\n",
      "Success: Image 4_1.png is for digit 4 and the inference result is 4.\n",
      "Success: Image 4_2.png is for digit 4 and the inference result is 4.\n",
      "Success: Image 4_3.png is for digit 4 and the inference result is 4.\n",
      "Success: Image 4_4.png is for digit 4 and the inference result is 4.\n",
      "Success: Image 5_0.png is for digit 5 and the inference result is 5.\n",
      "Success: Image 5_1.png is for digit 5 and the inference result is 5.\n",
      "Success: Image 5_2.png is for digit 5 and the inference result is 5.\n",
      "Success: Image 5_3.png is for digit 5 and the inference result is 5.\n",
      "Success: Image 5_4.png is for digit 5 and the inference result is 5.\n",
      "Success: Image 6_0.png is for digit 6 and the inference result is 6.\n",
      "Success: Image 6_1.png is for digit 6 and the inference result is 6.\n",
      "Success: Image 6_2.png is for digit 6 and the inference result is 6.\n",
      "Success: Image 6_3.png is for digit 6 and the inference result is 6.\n",
      "Success: Image 6_4.png is for digit 6 and the inference result is 6.\n",
      "Success: Image 7_0.png is for digit 7 and the inference result is 7.\n",
      "Success: Image 7_1.png is for digit 7 and the inference result is 7.\n",
      "Success: Image 7_2.png is for digit 7 and the inference result is 7.\n",
      "Success: Image 7_3.png is for digit 7 and the inference result is 7.\n",
      "Success: Image 7_4.png is for digit 7 and the inference result is 7.\n",
      "Success: Image 8_0.png is for digit 8 and the inference result is 8.\n",
      "Success: Image 8_1.png is for digit 8 and the inference result is 8.\n",
      "Success: Image 8_2.png is for digit 8 and the inference result is 8.\n",
      "Success: Image 8_3.png is for digit 8 and the inference result is 8.\n",
      "Success: Image 8_4.png is for digit 8 and the inference result is 8.\n",
      "Success: Image 9_0.png is for digit 9 and the inference result is 9.\n",
      "Success: Image 9_1.png is for digit 9 and the inference result is 9.\n",
      "Success: Image 9_2.png is for digit 9 and the inference result is 9.\n",
      "Success: Image 9_3.png is for digit 9 and the inference result is 9.\n",
      "Success: Image 9_4.png is for digit 9 and the inference result is 9.\n",
      "\n",
      "Handwritten set accuracy: 49/50 = 0.9800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 10:55:48.971132: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-02 10:55:51.468913: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-02 10:55:53.218435: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "!python module9.py --all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece5831-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
