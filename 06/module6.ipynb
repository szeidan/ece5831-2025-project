{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e2cb9f",
   "metadata": {},
   "source": [
    "# Neural Network Traiing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ad9bf",
   "metadata": {},
   "source": [
    "## Loss Function - Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648c571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packa\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76625938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is predicted value\n",
    "# t is true value (label)\n",
    "def cross_entroy_error(y, t):\n",
    "    delta = 1e-7  # To avoid log(0)\n",
    "    return -np.sum(t * np.log(y + delta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8949ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)  # prevent overflow\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26f88461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss between [0.19086542 0.42477881 0.38435576] and [0 1 0] = 0.856186451532546.\n",
      "Cross-Entropy Loss between [0.44694665 0.22194714 0.33110622] and [0 1 0] = 1.5053156021257363\n"
     ]
    }
   ],
   "source": [
    "# test the function\n",
    "y1 = np.array([0.1, 0.9, 0.8])\n",
    "y2 = np.array([0.8, 0.1, 0.5])\n",
    "\n",
    "y1_prob = softmax(y1)\n",
    "y2_prob = softmax(y2)\n",
    "\n",
    "t = np.array([0, 1, 0])\n",
    "loss1 = cross_entroy_error(y1_prob, t)\n",
    "print(f'Cross-Entropy Loss between {y1_prob} and {t} = {loss1}.')\n",
    "loss2 = cross_entroy_error(y2_prob, t)\n",
    "print(f'Cross-Entropy Loss between {y2_prob} and {t} = {loss2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7b483",
   "metadata": {},
   "source": [
    "## Mini-Batch Version of `cross_entropy_error()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2091cf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 shape: (3,) -> y1_prob shape: (3,)\n",
      "y2 shape: (3,) -> y2_prob shape: (3,)\n"
     ]
    }
   ],
   "source": [
    "# show what shape is changed\n",
    "print(f'y1 shape: {y1.shape} -> y1_prob shape: {y1_prob.shape}')\n",
    "print(f'y2 shape: {y2.shape} -> y2_prob shape: {y2_prob.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2610552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4497401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch version of cross_entropy_error\n",
    "def cross_entropy_error_batch(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7  # To avoid log(0)\n",
    "    return -np.sum(t * np.log(y + delta)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e2cede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 = [[0.1 0.9 0.8]\n",
      " [0.8 0.1 0.5]\n",
      " [0.2 0.3 0.5]]\n",
      "y2 = [[0.9 0.1 0.8]\n",
      " [0.8 0.1 0.5]\n",
      " [0.2 0.3 0.5]]\n",
      "y1_prob = [[0.19086542 0.42477881 0.38435576]\n",
      " [0.44694665 0.22194714 0.33110622]\n",
      " [0.28943311 0.31987306 0.39069383]]\n",
      "y2_prob = [[0.42477881 0.19086542 0.38435576]\n",
      " [0.44694665 0.22194714 0.33110622]\n",
      " [0.28943311 0.31987306 0.39069383]]\n",
      "t = [[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "y1 = np.array([[0.1, 0.9, 0.8],\n",
    "    [0.8, 0.1, 0.5],\n",
    "    [0.2, 0.3, 0.5]])\n",
    "y2 = np.array([[0.9, 0.1, 0.8],\n",
    "    [0.8, 0.1, 0.5],\n",
    "    [0.2, 0.3, 0.5]])\n",
    "y1_prob = np.apply_along_axis(softmax, 1, y1)\n",
    "y2_prob = np.apply_along_axis(softmax, 1, y2)\n",
    "\n",
    "t = np.array([[0, 1, 0],\n",
    "     [1, 0, 0],\n",
    "     [0, 0, 1]])\n",
    "\n",
    "print(f'y1 = {y1}')\n",
    "print(f'y2 = {y2}')\n",
    "print(f'y1_prob = {y1_prob}')\n",
    "print(f'y2_prob = {y2_prob}')\n",
    "print(f't = {t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cc3b72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss (batch) of y1_prob = 0.8671110284550632.\n",
      "Cross-Entropy Loss (batch) of y2_prob = 1.1337775989508578.\n"
     ]
    }
   ],
   "source": [
    "# test cross_entropy_error_batch\n",
    "loss1 = cross_entropy_error_batch(y1_prob, t)\n",
    "loss2 = cross_entropy_error_batch(y2_prob, t)\n",
    "print(f'Cross-Entropy Loss (batch) of y1_prob = {loss1}.')\n",
    "print(f'Cross-Entropy Loss (batch) of y2_prob = {loss2}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf0ea7",
   "metadata": {},
   "source": [
    "## Numberical Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3474a40",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfb7f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f60105fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a sample function\n",
    "def function_1(x):  \n",
    "    return x**2 + 0.1 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "307a2371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numerical differentiation of function_1 at x=0.2 is 0.49999999999994493.\n",
      "The numerical differentiation of function_1 at x=0.4 is 0.9000000000000674.\n"
     ]
    }
   ],
   "source": [
    "s2 = numerical_diff(function_1, 0.2)\n",
    "s4 = numerical_diff(function_1, 0.4)\n",
    "print(f'The numerical differentiation of function_1 at x=0.2 is {s2}.')\n",
    "print(f'The numerical differentiation of function_1 at x=0.4 is {s4}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a7467",
   "metadata": {},
   "source": [
    "## Partial Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "621b2043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The partial derivative with respect to x0 at (3, 4) is 6.00000000000378.\n",
      "The partial derivative with respect to x1 at (3, 4) is 7.999999999999119.\n"
     ]
    }
   ],
   "source": [
    "# Partial Derivatives when x0 = 3, x1 = 4\n",
    "\n",
    "def function_temp1(x0):\n",
    "    return x0**2 + 4.0**2\n",
    "\n",
    "\n",
    "def function_temp2(x1):\n",
    "    return 3.0**2 + x1**2  \n",
    "\n",
    "dx0 = numerical_diff(function_temp1, 3.0)\n",
    "dx1 = numerical_diff(function_temp2, 4.0)\n",
    "print(f'The partial derivative with respect to x0 at (3, 4) is {dx0}.')\n",
    "print(f'The partial derivative with respect to x1 at (3, 4) is {dx1}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576285e",
   "metadata": {},
   "source": [
    "## Slopes - `numerical_gradient()`\n",
    "\n",
    "Slopes are a vector of the partical derivaties of all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9fa5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)  # Initialize gradient array\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        # f(x + h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # f(x - h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val  # Restore original value\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "737e10e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numerical gradient of function_2 at (3.0, 4.0) is [6. 8.].\n",
      "The numerical gradient of function_2 at (0.0, 2.0) is [0. 4.].\n",
      "The numerical gradient of function_2 at (0.0, 0.0) is [0. 0.].\n"
     ]
    }
   ],
   "source": [
    "# test the function\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "grad = numerical_gradient(function_2, np.array([3.0, 4.0]))\n",
    "print(f'The numerical gradient of function_2 at (3.0, 4.0) is {grad}.') \n",
    "\n",
    "grad = numerical_gradient(function_2, np.array([0.0, 2.0]))\n",
    "print(f'The numerical gradient of function_2 at (0.0, 2.0) is {grad}.')\n",
    "\n",
    "grad = numerical_gradient(function_2, np.array([0.0, 0.0])) \n",
    "print(f'The numerical gradient of function_2 at (0.0, 0.0) is {grad}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15fb6d8",
   "metadata": {},
   "source": [
    "## Gradient Decent Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "762178af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2245ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "        # if grad.all() < 1e-5:\n",
    "        #     break\n",
    "\n",
    "    return x #, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4252c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2002f47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The local minimum point found by gradient descent is [1.65965717e-17 1.24478705e-17].\n"
     ]
    }
   ],
   "source": [
    "int_x = np.array([400.0, 300.0])\n",
    "x = gradient_descent(function_2, int_x, lr=0.1, step_num=200)\n",
    "print(f'The local minimum point found by gradient descent is {x}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27697915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The local minimum point found by gradient descent is [4.69649683e-09 3.52237262e-09].\n"
     ]
    }
   ],
   "source": [
    "x = gradient_descent(function_2, int_x, lr=0.0001, step_num=100)\n",
    "print(f'The local minimum point found by gradient descent is {x}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a61ea50",
   "metadata": {},
   "source": [
    "## SimpleNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utility:\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, a):\n",
    "        c = np.max(a)\n",
    "        exp_a = np.exp(a - c)  # prevent overflow\n",
    "        sum_exp_a = np.sum(exp_a)\n",
    "        y = exp_a / sum_exp_a\n",
    "        return y\n",
    "\n",
    "    def cross_entropy_error_batch(self, y, t):\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        batch_size = y.shape[0]\n",
    "        delta = 1e-7  # To avoid log(0)\n",
    "        return -np.sum(t * np.log(y + delta)) / batch_size\n",
    "    \n",
    "    def numerical_gradient(self, f, x):\n",
    "        h = 1e-4  # 0.0001\n",
    "        grad = np.zeros_like(x)  # Initialize gradient array\n",
    "\n",
    "        for idx in range(x.size):\n",
    "            tmp_val = x[idx]\n",
    "\n",
    "            # f(x + h)\n",
    "            x[idx] = tmp_val + h\n",
    "            fxh1 = f(x)\n",
    "\n",
    "            # f(x - h)\n",
    "            x[idx] = tmp_val - h\n",
    "            fxh2 = f(x)\n",
    "\n",
    "            grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "            x[idx] = tmp_val  # Restore original value\n",
    "\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3b2080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility \n",
    " \n",
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)  # weight initialization\n",
    "        self.util = Utility()\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = self.util.softmax(z)\n",
    "        loss = self.util.cross_entropy_error_batch(y, t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e940a2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight W: [[ 2.50649385  1.9619885  -0.01547902]\n",
      " [ 1.65268115  0.71875699  0.62257074]]\n",
      "Predicted value: [2.99130935 1.82407439 0.55102625]\n",
      "Loss value: 2.7755833883781973.\n"
     ]
    }
   ],
   "source": [
    "# test the SimpleNet class\n",
    "net = SimpleNet()\n",
    "print(f'Initial weight W: {net.W}')\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(f'Predicted value: {p}')\n",
    "t = np.array([0, 0, 1])  # true label\n",
    "loss = net.loss(x, t)\n",
    "print(f'Loss value: {loss}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91092e3f",
   "metadata": {},
   "source": [
    "## Two-Layer Net for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "211a11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utility import Utility\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Weight initialization\n",
    "        self.params = {}\n",
    "        self.params['w1'] = np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['w2'] = np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        self.util = Utility()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        w1, b1 = self.params['w1'], self.params['b1']\n",
    "        w2, b2 = self.params['w2'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, w1) + b1\n",
    "        z1 = self.util.sigmoid(a1)\n",
    "        a2 = np.dot(z1, w2) + b2\n",
    "        y = self.util.softmax(a2)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):   \n",
    "        y_hat = self.predict(x)\n",
    "        return self.util.cross_entropy_error_batch(y_hat, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y_hat = self.predict(x)\n",
    "        y_pred = np.argmax(y_hat, axis=1)\n",
    "        t_true = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y_pred == t_true) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['w1'] = self.util.numerical_gradient(loss_W, self.params['w1'])\n",
    "        grads['b1'] = self.util.numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['w2'] = self.util.numerical_gradient(loss_W, self.params['w2'])\n",
    "        grads['b2'] = self.util.numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f496ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "train-images-idx3-ubyte.gz already exists. Skipping download.\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "train-labels-idx1-ubyte.gz already exists. Skipping download.\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "t10k-images-idx3-ubyte.gz already exists. Skipping download.\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "t10k-labels-idx1-ubyte.gz already exists. Skipping download.\n",
      "mnist.pkl already exists. Loading dataset from pickle file.\n"
     ]
    }
   ],
   "source": [
    "#import mnist_data\n",
    "from mnist_data import MnistData\n",
    "\n",
    "mnist = MnistData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae04e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the Two-Layer Net for MNIST\n",
    "net = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "522735c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted train_labels -> (60000, 10), test_labels -> (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# convert train_labels and test_labels to one-hot encoding\n",
    "num_classes = 10\n",
    "\n",
    "def to_one_hot(labels, num_classes):\n",
    "    labels = np.array(labels).reshape(-1)\n",
    "    return np.eye(num_classes, dtype=np.float32)[labels]\n",
    "\n",
    "if train_labels.ndim != 2 or train_labels.shape[1] != num_classes:\n",
    "    train_labels = to_one_hot(train_labels, num_classes)\n",
    "\n",
    "if test_labels.ndim != 2 or test_labels.shape[1] != num_classes:\n",
    "    test_labels = to_one_hot(test_labels, num_classes)\n",
    "\n",
    "print(f\"Converted train_labels -> {train_labels.shape}, test_labels -> {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bde0fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53d04843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's flat the images\n",
    "train_images = train_images.reshape(train_images.shape[0], 784)\n",
    "test_images = test_images.reshape(test_images.shape[0], 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c74de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c54e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12020d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 100  #10,000 preferably \n",
    "train_size = int(train_images.shape[0]/6) # make this 1/6th for faster run\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2034a626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "278f6140",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e3ba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Train Accuracy = 0.10905, Test Accuracy = 0.099\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m train_labels[batch_mask]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Gradient calculation\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Parameter update\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     38\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[1;32m     40\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 41\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mnumerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     43\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mnumerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/ece5831-2025-code/06/utility.py:33\u001b[0m, in \u001b[0;36mUtility.numerical_gradient\u001b[0;34m(self, f, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m original_value \u001b[38;5;241m=\u001b[39m x[idx]\n\u001b[1;32m     32\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m original_value \u001b[38;5;241m+\u001b[39m h\n\u001b[0;32m---> 33\u001b[0m fxh1 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# f(x + h)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m original_value \u001b[38;5;241m-\u001b[39m h\n\u001b[1;32m     36\u001b[0m fxh2 \u001b[38;5;241m=\u001b[39m f(x)  \u001b[38;5;66;03m# f(x - h)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m---> 38\u001b[0m     loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     41\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mnumerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):   \n\u001b[0;32m---> 26\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mcross_entropy_error_batch(y_hat, t)\n",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m, in \u001b[0;36mTwoLayerNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m w1, b1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m w2, b2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[1;32m     19\u001b[0m z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39msigmoid(a1)\n\u001b[1;32m     20\u001b[0m a2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(z1, w2) \u001b[38;5;241m+\u001b[39m b2\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(iter_num):\n",
    "    # Mini-batch selection\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = train_images[batch_mask]\n",
    "    t_batch = train_labels[batch_mask]\n",
    "\n",
    "    # Gradient calculation\n",
    "    grad = net.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    # Parameter update\n",
    "    for key in ('w1', 'b1', 'w2', 'b2'):\n",
    "        net.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # Record the loss\n",
    "    loss = net.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # Evaluate accuracy at each epoch\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = net.accuracy(train_images, train_labels)\n",
    "        test_acc = net.accuracy(test_images, test_labels)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Iteration {i}: Train Accuracy = {train_acc}, Test Accuracy = {test_acc}\")\n",
    "\n",
    "\n",
    "\n",
    "# plot the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.plot(x, train_acc_list, label='Train Accuracy')\n",
    "plt.plot(x, test_acc_list, label='Test Accuracy', linestyle='--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece5831-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}